% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/clustering_pipeline.R
\name{clusteval_scoring}
\alias{clusteval_scoring}
\alias{scoring}
\alias{get_best_result}
\title{Scoring of dimensionality reduction and clustering pipeline output}
\usage{
clusteval_scoring(...)

scoring(
  res,
  by = c("datname", "drname", "k", "m", "mkkm_mr_lambda"),
  wsum = TrainStabilityJaccard + Silhouette,
  significance_level = 0.05,
  summarise = TRUE,
  format_names = TRUE
)

get_best_result(res, scores)
}
\arguments{
\item{...}{}

\item{res}{result from \code{\link{COPS}}}

\item{by}{character vector containing column names to group analysis by}

\item{wsum}{an expression that indicates how a combined score is computed}

\item{significance_level}{p-value cutoff for computing rejection rates}

\item{summarise}{If FALSE, adds \code{"run"} and \code{"fold"} to \code{by}. 
By default the metrics are averaged across runs and folds.}

\item{format_names}{If TRUE, formats internally used method names etc. to more 
user friendly names.}

\item{scores}{scores from \code{\link{scoring}}}
}
\value{
Returns a \code{list} containing a \code{data.frame} \code{$all} of all scores and
        a single row \code{$best} with the best score according to \code{wsum}.
}
\description{
Computes averages of metrics from pipeline output and also returns the
best combination based on a weighted sum of metrics.
}
\details{
Metrics are renamed for convenience: 
\itemize{
  \item [Train/Test]Stability[Jaccard/ARI/NMI]
  \item [NMI/ARI/ChisqRR].<batch>
  \item [NMI/ARI].<subtype>
  \item ...
}
}
\section{Functions}{
\itemize{
\item \code{clusteval_scoring()}: Alias for scoring

\item \code{get_best_result()}: Retrieves best clustering from CV results based on scores. 
In practice retrieves reference fold result from first run matching the best results.

}}
\examples{
library(COPS)
library(parallel)

res <- COPS(ad_ge_micro_zscore, 
association_data = ad_studies, 
parallel = 2, nruns = 2, nfolds = 5, 
dimred_methods = c("pca", "umap", "tsne"), 
cluster_methods = c("hierarchical", "kmeans"), 
distance_metric = "euclidean",
n_clusters = 2:4)

scores <- scoring(res, wsum = Silhouette - GSE.nmi, summarise = TRUE)

best <- get_best_result(res, scores)
head(best$embedding)
head(best$clusters)

}
