train_stability = out_train,
valid_stability = out_valid,
mean_silhouette = mean_silhouette,
batch_effect = unlist(lapply(batch_effect, function(x) mean(unlist(x) < significance))),
DSC = dsc_vals)#,
#kBET = kBET_vals)
nc
u
clust[[u]] <- KMeans_rcpp(train_list[[u]], nc, 100)
nruns <- length(train_list)
out_train <- c()
out_valid <- c()
mean_silhouette <- c()
batch_effect <- list()
for (nc in k_lb:k_ub) {
clust <- list()
clust_train <- list()
clust_valid <- list()
batch_effect[[nc]] <- list()
silh <- list()
for (u in 1:nruns) {
clust[[u]] <- KMeans_rcpp(train_list[[u]], nc, 100)
clust_valid[[u]] <- predict_KMeans(valid_list[[u]], clust[[u]]$centroids )
clust_train[[u]] <- clust[[u]]$clusters
# Batch effect
# The table may be a little too sparse for chi-squared test
# Maybe Fisher's exact test with simulated p.values would be more appropriate
suppressWarnings(
batch_effect[[nc]][u] <- chisq.test(table(
c(train_batch, valid_batch),
c(clust_train[[u]], clust_valid[[u]])))$p.value
)
# Fisher's exact test - returns almost identical p-values most of the time
#batch_effect[[nc]][[u]] <- fisher.test(table(
#  c(train_batch, valid_batch),
#  c(clust_train[[u]], clust_valid[[u]])),
#  simulate.p.value = TRUE)$p.value
dm <- distance_matrix(
rbind(train_list[[u]], valid_list[[u]]),
method ='euclidean',
upper = TRUE,
diagonal = TRUE)
silh[[u]] <- summary(silhouette(c(clust_train[[u]], clust_valid[[u]]), dmatrix = dm))$avg.width
}
out_train[nc] <- jdist(nruns, nc, clust_train)
out_valid[nc] <- jdist(nruns, nc, clust_valid)
mean_silhouette[nc] <- mean(unlist(silh))
}
u
nc
KMeans_rcpp(train_list[[u]], nc, 100)
KMeans_rcpp(train_list[[u]], nc, 100)
KMeans_rcpp(train_list[[u]], nc, 100)
train_list[[u]]
any(is.na(train_list[[u]]))
any(is.nan(train_list[[u]]))
nc
dim(train_list[[u]])
apply(train_list[[u]], 1, var)
apply(train_list[[u]], 2, var)
apply(train_list[[u]], 2, mean)
apply(train_list[[1]], 2, mean)
apply(train_list[[2]], 2, mean)
apply(train_list[[3]], 2, mean)
apply(train_list[[4]], 2, mean)
apply(train_list[[5]], 2, mean)
apply(train_list[[5]], 2, var)
apply(train_list[[2]], 2, var)
apply(train_list[[3]], 2, var)
apply(train_list[[4]], 2, var)
valid_list[[1]]
apply(rbind(train_list[[1]],valid_list[[1]]), 2, var)
## Loader for all files related to the parameter search
psearch_loader <- function(file_path, file_prefix) {
cv_ind <- read.table(paste0(file_path, "\\", file_prefix, "_cv_ind.csv"), header = FALSE, sep = ",")
#j <- 1 # cross validation fold ID (0-4) -- NOT IMPLEMENTED
parameters <- read.table(paste0(file_path, "\\", file_prefix, "_parameters.csv"), header = FALSE, sep = ",")
colnames(parameters) <- c("L1", "L2", "L3", "L4", "LRATE", "WREG", "NOISE", "MMDREG", "MMDSIGMA", "PATIENCE")
train_embed <- read.table(paste0(file_path, "\\", file_prefix, "_train_embed.csv"), header = FALSE, sep = ",")
train_run <- read.table(paste0(file_path, "\\", file_prefix, "_train_run.csv"), header = FALSE, sep = ",")
valid_embed <- read.table(paste0(file_path, "\\", file_prefix, "_valid_embed.csv"), header = FALSE, sep = ",")
valid_run <- read.table(paste0(file_path, "\\", file_prefix, "_valid_run.csv"), header = FALSE, sep = ",")
# All embeddings associated with one set of parameters
# Also clean any data sets with missing values
train_embed_k <- list()
train_run_k <- list()
train_k_ind <- c(0, which(diff(train_run[[1]]) < 0), nrow(train_run))
valid_embed_k <- list()
valid_run_k <- list()
valid_k_ind <- c(0, which(diff(valid_run[[1]]) < 0), nrow(valid_run))
removed_ind <- rep(FALSE, nrow(parameters))
for (k in 1:(length(train_k_ind)-1)) {
if (k > length(valid_k_ind) - 1) {
#removed_ind[k] <- TRUE
stop("valid run index does not match training run index by length")
} else {
temp_embed <- train_embed[(train_k_ind[k]+1):train_k_ind[k+1],1:parameters$L4[k]]
temp_run <- train_run[(train_k_ind[k]+1):train_k_ind[k+1],1]
temp_embed_val <- valid_embed[(valid_k_ind[k]+1):valid_k_ind[k+1],1:parameters$L4[k]]
temp_run_val <- valid_run[(valid_k_ind[k]+1):valid_k_ind[k+1],1]
if (!(any(is.na(temp_embed)) |
any(is.na(temp_run)) |
any(is.na(temp_embed_val)) |
any(is.na(temp_run_val))) &
(nrow(temp_embed) > 0 &
nrow(temp_embed_val) > 0 &
length(temp_run) > 0 &
length(temp_run_val) > 0)) {
if (any(apply(temp_embed, 2, var) > 0) |
any(apply(temp_embed_val, 2, var) > 0)) {
train_embed_k[[k]] <- temp_embed
train_run_k[[k]] <- temp_run
valid_embed_k[[k]] <- temp_embed_val
valid_run_k[[k]] <- temp_run_val
} else {
removed_ind[k] <- TRUE
warning("Removed search iteration with 0 variance in embeddings")
}
} else {
# scheme to remove corresponding parameter rows or create index of valid runs
removed_ind[k] <- TRUE
warning("Removed search iteration with nan data or 0 runs")
}
}
}
#parameters <- parameters[!removed_ind,]
return(list(parameters = parameters[!removed_ind,],
train_embed_k = train_embed_k[!removed_ind],
train_run_k = train_run_k[!removed_ind],
valid_embed_k = valid_embed_k[!removed_ind],
valid_run_k = valid_run_k[!removed_ind],
cv_ind = cv_ind))
}
psdata <- psearch_loader(file_path, file_prefix)
parameters <- psdata$parameters
train_embed_k <- psdata$train_embed_k
train_run_k <- psdata$train_run_k
valid_embed_k <- psdata$valid_embed_k
valid_run_k <- psdata$valid_run_k
cv_ind <- psdata$cv_ind
cv_fold_ind <- 1 #j
cv_ind_j <- (cv_ind + 1) != cv_fold_ind
emb_evals <- list()
par_run_evals <- data.frame()
# For every parameter set k in parallel run i fold j
for (k in 1:length(train_embed_k)) {
emb_evals[[k]] <- evaluate_embeddings(
split(train_embed_k[[k]], train_run_k[[k]]),
split(valid_embed_k[[k]], valid_run_k[[k]]),
batch[cv_ind_j],
batch[!cv_ind_j],
2,
10)
par_run_evals <- rbind(par_run_evals,
suppressWarnings(data.frame(
parameters[k,],
as.data.frame(emb_evals[[k]]),
clusters = 1:10)))
}
k
warnings()
apply(train_embed_k, 2, var)
dim(train_embed_k)
length(train_embed_k)
dim(train_embed_k[[k]])
apply(train_embed_k[[k]], 2, var)
apply(split(train_embed_k[[k]], train_run_k[[k]])[[1]] 2, var)
apply(split(train_embed_k[[k]], train_run_k[[k]])[[1]], 2, var)
apply(split(train_embed_k[[k]], train_run_k[[k]])[[2]], 2, var)
apply(split(train_embed_k[[k]], train_run_k[[k]])[[3]], 2, var)
apply(split(train_embed_k[[k]], train_run_k[[k]])[[4]], 2, var)
apply(split(train_embed_k[[k]], train_run_k[[k]])[[5]], 2, var)
library(ppclust)
library(fclust)
library(cluster)
## Loader for all files related to the parameter search
psearch_loader <- function(file_path, file_prefix) {
cv_ind <- read.table(paste0(file_path, "\\", file_prefix, "_cv_ind.csv"), header = FALSE, sep = ",")
#j <- 1 # cross validation fold ID (0-4) -- NOT IMPLEMENTED
parameters <- read.table(paste0(file_path, "\\", file_prefix, "_parameters.csv"), header = FALSE, sep = ",")
colnames(parameters) <- c("L1", "L2", "L3", "L4", "LRATE", "WREG", "NOISE", "MMDREG", "MMDSIGMA", "PATIENCE")
train_embed <- read.table(paste0(file_path, "\\", file_prefix, "_train_embed.csv"), header = FALSE, sep = ",")
train_run <- read.table(paste0(file_path, "\\", file_prefix, "_train_run.csv"), header = FALSE, sep = ",")
valid_embed <- read.table(paste0(file_path, "\\", file_prefix, "_valid_embed.csv"), header = FALSE, sep = ",")
valid_run <- read.table(paste0(file_path, "\\", file_prefix, "_valid_run.csv"), header = FALSE, sep = ",")
# All embeddings associated with one set of parameters
# Also clean any data sets with missing values
train_embed_k <- list()
train_run_k <- list()
train_k_ind <- c(0, which(diff(train_run[[1]]) < 0), nrow(train_run))
valid_embed_k <- list()
valid_run_k <- list()
valid_k_ind <- c(0, which(diff(valid_run[[1]]) < 0), nrow(valid_run))
removed_ind <- rep(FALSE, nrow(parameters))
for (k in 1:(length(train_k_ind)-1)) {
if (k > length(valid_k_ind) - 1) {
#removed_ind[k] <- TRUE
stop("valid run index does not match training run index by length")
} else {
temp_embed <- train_embed[(train_k_ind[k]+1):train_k_ind[k+1],1:parameters$L4[k]]
temp_run <- train_run[(train_k_ind[k]+1):train_k_ind[k+1],1]
temp_embed_val <- valid_embed[(valid_k_ind[k]+1):valid_k_ind[k+1],1:parameters$L4[k]]
temp_run_val <- valid_run[(valid_k_ind[k]+1):valid_k_ind[k+1],1]
if (!(any(is.na(temp_embed)) |
any(is.na(temp_run)) |
any(is.na(temp_embed_val)) |
any(is.na(temp_run_val))) &
(nrow(temp_embed) > 0 &
nrow(temp_embed_val) > 0 &
length(temp_run) > 0 &
length(temp_run_val) > 0)) {
run_embedding_0var <- function(x,xi) sapply(split(x, xi), function(y) all(apply(y, 2, var) == 0))
if (any(run_embedding_0var(temp_embed, temp_run)) |
any(run_embedding_0var(temp_embed_val, temp_run_val))) {
train_embed_k[[k]] <- temp_embed
train_run_k[[k]] <- temp_run
valid_embed_k[[k]] <- temp_embed_val
valid_run_k[[k]] <- temp_run_val
} else {
removed_ind[k] <- TRUE
warning("Removed search iteration with 0 variance in embeddings")
}
} else {
# scheme to remove corresponding parameter rows or create index of valid runs
removed_ind[k] <- TRUE
warning("Removed search iteration with nan data or 0 runs")
}
}
}
#parameters <- parameters[!removed_ind,]
return(list(parameters = parameters[!removed_ind,],
train_embed_k = train_embed_k[!removed_ind],
train_run_k = train_run_k[!removed_ind],
valid_embed_k = valid_embed_k[!removed_ind],
valid_run_k = valid_run_k[!removed_ind],
cv_ind = cv_ind))
}
psdata <- psearch_loader(file_path, file_prefix)
warnings()
length(psdata$parameters)
run_embedding_0var <- function(x,xi) sapply(split(x, xi), function(y) all(apply(y, 2, var) == 0))
run_embedding_0var(temp_embed, temp_run)
cv_ind <- read.table(paste0(file_path, "\\", file_prefix, "_cv_ind.csv"), header = FALSE, sep = ",")
#j <- 1 # cross validation fold ID (0-4) -- NOT IMPLEMENTED
parameters <- read.table(paste0(file_path, "\\", file_prefix, "_parameters.csv"), header = FALSE, sep = ",")
colnames(parameters) <- c("L1", "L2", "L3", "L4", "LRATE", "WREG", "NOISE", "MMDREG", "MMDSIGMA", "PATIENCE")
train_embed <- read.table(paste0(file_path, "\\", file_prefix, "_train_embed.csv"), header = FALSE, sep = ",")
train_run <- read.table(paste0(file_path, "\\", file_prefix, "_train_run.csv"), header = FALSE, sep = ",")
valid_embed <- read.table(paste0(file_path, "\\", file_prefix, "_valid_embed.csv"), header = FALSE, sep = ",")
valid_run <- read.table(paste0(file_path, "\\", file_prefix, "_valid_run.csv"), header = FALSE, sep = ",")
# All embeddings associated with one set of parameters
# Also clean any data sets with missing values
train_embed_k <- list()
train_run_k <- list()
train_k_ind <- c(0, which(diff(train_run[[1]]) < 0), nrow(train_run))
valid_embed_k <- list()
valid_run_k <- list()
valid_k_ind <- c(0, which(diff(valid_run[[1]]) < 0), nrow(valid_run))
removed_ind <- rep(FALSE, nrow(parameters))
for (k in 1:(length(train_k_ind)-1)) {
if (k > length(valid_k_ind) - 1) {
#removed_ind[k] <- TRUE
stop("valid run index does not match training run index by length")
} else {
temp_embed <- train_embed[(train_k_ind[k]+1):train_k_ind[k+1],1:parameters$L4[k]]
temp_run <- train_run[(train_k_ind[k]+1):train_k_ind[k+1],1]
temp_embed_val <- valid_embed[(valid_k_ind[k]+1):valid_k_ind[k+1],1:parameters$L4[k]]
temp_run_val <- valid_run[(valid_k_ind[k]+1):valid_k_ind[k+1],1]
if (!(any(is.na(temp_embed)) |
any(is.na(temp_run)) |
any(is.na(temp_embed_val)) |
any(is.na(temp_run_val))) &
(nrow(temp_embed) > 0 &
nrow(temp_embed_val) > 0 &
length(temp_run) > 0 &
length(temp_run_val) > 0)) {
run_embedding_0var <- function(x,xi) sapply(split(x, xi), function(y) all(apply(y, 2, var) == 0))
if (any(run_embedding_0var(temp_embed, temp_run)) |
any(run_embedding_0var(temp_embed_val, temp_run_val))) {
train_embed_k[[k]] <- temp_embed
train_run_k[[k]] <- temp_run
valid_embed_k[[k]] <- temp_embed_val
valid_run_k[[k]] <- temp_run_val
} else {
removed_ind[k] <- TRUE
stop("Removed search iteration with 0 variance in embeddings")
}
} else {
# scheme to remove corresponding parameter rows or create index of valid runs
removed_ind[k] <- TRUE
warning("Removed search iteration with nan data or 0 runs")
}
}
}
k
run_embedding_0var(temp_embed, temp_run)
run_embedding_0var(temp_embed_val, temp_run_val))
run_embedding_0var(temp_embed_val, temp_run_val)
## Loader for all files related to the parameter search
psearch_loader <- function(file_path, file_prefix) {
cv_ind <- read.table(paste0(file_path, "\\", file_prefix, "_cv_ind.csv"), header = FALSE, sep = ",")
#j <- 1 # cross validation fold ID (0-4) -- NOT IMPLEMENTED
parameters <- read.table(paste0(file_path, "\\", file_prefix, "_parameters.csv"), header = FALSE, sep = ",")
colnames(parameters) <- c("L1", "L2", "L3", "L4", "LRATE", "WREG", "NOISE", "MMDREG", "MMDSIGMA", "PATIENCE")
train_embed <- read.table(paste0(file_path, "\\", file_prefix, "_train_embed.csv"), header = FALSE, sep = ",")
train_run <- read.table(paste0(file_path, "\\", file_prefix, "_train_run.csv"), header = FALSE, sep = ",")
valid_embed <- read.table(paste0(file_path, "\\", file_prefix, "_valid_embed.csv"), header = FALSE, sep = ",")
valid_run <- read.table(paste0(file_path, "\\", file_prefix, "_valid_run.csv"), header = FALSE, sep = ",")
# All embeddings associated with one set of parameters
# Also clean any data sets with missing values
train_embed_k <- list()
train_run_k <- list()
train_k_ind <- c(0, which(diff(train_run[[1]]) < 0), nrow(train_run))
valid_embed_k <- list()
valid_run_k <- list()
valid_k_ind <- c(0, which(diff(valid_run[[1]]) < 0), nrow(valid_run))
removed_ind <- rep(FALSE, nrow(parameters))
for (k in 1:(length(train_k_ind)-1)) {
if (k > length(valid_k_ind) - 1) {
#removed_ind[k] <- TRUE
stop("valid run index does not match training run index by length")
} else {
temp_embed <- train_embed[(train_k_ind[k]+1):train_k_ind[k+1],1:parameters$L4[k]]
temp_run <- train_run[(train_k_ind[k]+1):train_k_ind[k+1],1]
temp_embed_val <- valid_embed[(valid_k_ind[k]+1):valid_k_ind[k+1],1:parameters$L4[k]]
temp_run_val <- valid_run[(valid_k_ind[k]+1):valid_k_ind[k+1],1]
if (!(any(is.na(temp_embed)) |
any(is.na(temp_run)) |
any(is.na(temp_embed_val)) |
any(is.na(temp_run_val))) &
(nrow(temp_embed) > 0 &
nrow(temp_embed_val) > 0 &
length(temp_run) > 0 &
length(temp_run_val) > 0)) {
run_embedding_0var <- function(x,xi) sapply(split(x, xi), function(y) all(apply(y, 2, var) == 0))
if (any(run_embedding_0var(temp_embed, temp_run)) |
any(run_embedding_0var(temp_embed_val, temp_run_val))) {
removed_ind[k] <- TRUE
warning("Removed search iteration with 0 variance in embeddings")
} else {
train_embed_k[[k]] <- temp_embed
train_run_k[[k]] <- temp_run
valid_embed_k[[k]] <- temp_embed_val
valid_run_k[[k]] <- temp_run_val
}
} else {
# scheme to remove corresponding parameter rows or create index of valid runs
removed_ind[k] <- TRUE
warning("Removed search iteration with nan data or 0 runs")
}
}
}
#parameters <- parameters[!removed_ind,]
return(list(parameters = parameters[!removed_ind,],
train_embed_k = train_embed_k[!removed_ind],
train_run_k = train_run_k[!removed_ind],
valid_embed_k = valid_embed_k[!removed_ind],
valid_run_k = valid_run_k[!removed_ind],
cv_ind = cv_ind))
}
psdata <- psearch_loader(file_path, file_prefix)
warnings()
tryCatch({
ps_out[[ps_file_index]] <- psearch_eval(file_path, "GPUsearch200120", batch)
ps_file_index <- ps_file_index + 1
}, error = function(e) print(paste("Run", i, e)))
tryCatch({
ps_out[[ps_file_index]] <- psearch_eval(file_path, "GPUsearch200121", batch)
ps_file_index <- ps_file_index + 1
}, error = function(e) print(paste("Run", i, e)))
tryCatch({
ps_out[[ps_file_index]] <- psearch_eval(file_path, "GPUsearch200124", batch)
ps_file_index <- ps_file_index + 1
}, error = function(e) print(paste("Run", i, e)))
tryCatch({
ps_out[[ps_file_index]] <- psearch_eval(file_path, "GPUsearch200127", batch)
ps_file_index <- ps_file_index + 1
}, error = function(e) print(paste("Run", i, e)))
tryCatch({
ps_out[[ps_file_index]] <- psearch_eval(file_path, "GPUsearch200130", batch)
ps_file_index <- ps_file_index + 1
}, error = function(e) print(paste("Run", i, e)))
ps_out_all <- Reduce("rbind", ps_out)
ps_out_all <- ps_out_all[ps_out_all$clusters > 1, ]
ps_out_all$train_stability <- 1 - ps_out_all$train_stability
ps_out_all$valid_stability <- 1 - ps_out_all$valid_stability
nrow(ps_out_all)
ggplot(ps_out_all, aes(clusters, train_stability, group = clusters)) + geom_boxplot() + theme_bw()
library(ggplot)
library(ggplot2)
ggplot(ps_out_all, aes(clusters, train_stability, group = clusters)) + geom_boxplot() + theme_bw()
ggplot(ps_out_all, aes(clusters, valid_stability, group = clusters)) + geom_boxplot() + theme_bw()
ggplot(ps_out_all, aes(clusters, mean_silhouette, group = clusters)) + geom_boxplot() + theme_bw()
ggplot(ps_out_all, aes(clusters, batch_effect, group = clusters)) + geom_count() + theme_bw()
ggplot(ps_out_all, aes(clusters, train_stability - 0.5*batch_effect, group = clusters)) + geom_boxplot() + theme_bw()
ggplot(ps_out_all, aes(train_stability, mean_silhouette, color = clusters)) +
geom_point(shape = "+", size = 5) + theme_bw() + scale_color_distiller(palette="Spectral")
geom_point(shape = "+", size = 5, alpha = 0.6) + theme_bw() + scale_color_distiller(palette="Spectral")
geom_point(shape = "+", size = 5, alpha = 0.6) + theme_bw() +
facet_wrap(~clusters)
ggplot(ps_out_all, aes(batch_effect, train_stability, color = clusters)) +
geom_point(shape = "+", size = 5, alpha = 0.6) + theme_bw() + scale_color_distiller(palette="Spectral")
ggplot(ps_out_all, aes(batch_effect, train_stability)) +
geom_point(shape = "+", size = 5, alpha = 0.6) + theme_bw() +
facet_wrap(~clusters)
ggplot(ps_out_all, aes(kBET, train_stability)) +
geom_point(shape = "+", size = 5, alpha = 0.6) + theme_bw() +
facet_wrap(~clusters)
ggplot(ps_out_all, aes(DSC, train_stability)) +
geom_point(shape = "+", size = 5, alpha = 0.6) + theme_bw() +
facet_wrap(~clusters) + geom_vline(xintercept = 0.5, color = "red")
ggplot(ps_out_all, aes(train_stability, mean_silhouette, color = factor(L4))) +
geom_point(shape = "+", size = 5) + theme_bw() + scale_color_brewer(palette="Accent")
ggplot(ps_out_all, aes(train_stability, stat(count), fill = factor(batch_effect))) +
geom_histogram(position="stack") + scale_fill_brewer(palette="PuOr") + theme_bw() +
facet_grid(L4~clusters) + ggtitle("Stability vs Batch effect [number of clusters(2:10) x embedding dimension size(2,5,10)]")
ggplot(ps_out_all[ps_out_all$clusters < 6,], aes(DSC, train_stability, color = log(MMDREG))) +
geom_point(shape = "+", size = 5, alpha = 1.0) + theme_bw() +
facet_wrap(~clusters, labeller = label_both, scale = "free") +
geom_vline(xintercept = 0.5, color = "red") +
scale_color_distiller(palette = "YlGn") +
ggtitle("Stability vs DSC of parameter settings searched") +
ylab("Stability") + xlab("Dispersion Separability Criterion")
ggplot(ps_out_all[ps_out_all$clusters < 6,], aes(batch_effect, DSC, group = batch_effect)) +
geom_boxplot() + theme_bw() +
facet_wrap(~clusters, labeller = label_both, scale = "free") +
geom_hline(yintercept = 0.5, color = "red") +
ggtitle("Chisq test rejection rate vs DSC") +
ylab("Dispersion Separability Criterion") + xlab("Chisq test rejection rate") +
scale_x_reverse() + coord_flip()
ggplot(ps_out_all, aes(MMDREG, DSC)) + geom_bin2d(bins = 5) + theme_bw() +
facet_wrap(~clusters) + ggtitle("MMD regularization vs batch effect with different number of clusters")
ggplot(ps_out_all, aes(log10(MMDREG), DSC)) + geom_bin2d(bins = 5) + theme_bw() +
facet_wrap(~clusters) + ggtitle("MMD regularization vs batch effect with different number of clusters")
ggplot(ps_out_all, aes(log10(MMDREG), DSC)) + geom_bin2d(bins = 10) + theme_bw() +
facet_wrap(~clusters) + ggtitle("MMD regularization vs batch effect with different number of clusters")
ggplot(ps_out_all, aes(MMDSIGMA, DSC)) + geom_bin2d(bins = 10) + theme_bw() +
facet_wrap(~clusters) + ggtitle("MMD sigma vs batch effect with different number of clusters")
update.packages()
update.packages()
install.packages("clValid")
old.packages()
library(plyr)
setwd("~/GitHub/COPS")
devtools::build()
install.packages("devtools")
devtools::build()
install.packages("knitr")
devtools::build()
devtools::install_dev_deps()
if (!requireNamespace("BiocManager", quietly = TRUE))
install.packages("BiocManager")
BiocManager::install(version = "3.10")
BiocManager::install()
BiocManager::install("kBET")
BiocManager::install(version = "devel")
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(COPS)
library(swamp)
method_evaluations <- dimred_clusteval_pipeline(list(ad_ge_micro_zscore), batch_label = ad_studies, include_original = FALSE,
parallel = 4, nruns = 2,
nfolds = 2, dimred_methods = c("pca", "umap"),
cluster_methods = c("hierarchical", "kmeans"),
metric = "euclidean",
n_clusters = 2:4)
scores <- clusteval_scoring(method_evaluations)
# List scores
head(scores$all)
scores$best
# Generate embedding and clustering based on best setting
best <- dimred_cluster(list(ad_ge_micro_zscore), scores$best)
getwd()
devtools::build()
swamp::confounding(data.frame(best$embedding, factor(best$clustering), ad_studies))
class_associations(t(best$embedding), cbind(clust = best$clustering, study = ad_studies))
class_associations(list(ad_ge_micro_zscore)[[scores$best$datname]],
cbind(clust = best$clustering, study = ad_studies))
write.csv(t(ad_ge_micro_zscore), file = "ad_gexpr.csv")
write.csv(ad_studies, file = "ad_batches.csv")
devtools::build_vignette()
devtools::clean_vignettes()
devtools::build()
320000 / 80
BiocManager::install("TCGAbiolinks")
brca <- curatedTCGAData::curatedTCGAData(diseaseCode = "BRCA", assays = "RNASeqGene", dry.run = FALSE)
# Clinical data
dim(brca@colData)
colnames(brca@colData)[!grepl(paste("^patient.drug",
"^patient.radiations",
"^patient.biospecimen_cqcf",
"^patient.follow_ups",
"^patient.samples",
sep = "|"),colnames(brca@colData))]
#require(magrittr)
colnames(brca@colData)[grep("plate", colnames(brca@colData))] #%>% subset(grep("plate"))
#table(brca@colData$patient.samples.sample.portions.shipment_portion.plate_id,
#      brca@colData$patient.samples.sample.portions.shipment_portion.2.plate_id,
#      useNA = "ifany")
# Some controls/healthy tissue
dup_ind <- duplicated(sampleMap(brca)$primary)
library(TCGAutils)
tbrca <- splitAssays(brca)
brca_sample_type <- rep(names(tbrca), sapply(experiments(tbrca), function(x) ncol(assays(x)[[1]])))
tbrca <- Reduce("cbind", lapply(experiments(tbrca), function(x) assays(x)[[1]]))
strsplit(colnames(tbrca), "-")
brca_batch <- sapply(strsplit(colnames(tbrca), "-"), function(x) x[7])
table(brca_batch)
?clusterProfiler::GSEA
?clusterProfiler::enricher
